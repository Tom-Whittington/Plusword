{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c84ad7-9fe9-4395-8f58-8da283b95747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime as dt\n",
    "import json\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3ffa0-5926-4fc1-a28a-2d08514bf63a",
   "metadata": {},
   "source": [
    "### List of mumsnet urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6d43bd-616b-487f-9a52-434664ab1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [#'https://www.mumsnet.com/talk/am_i_being_unreasonable/4676538-if-you-like-wordle-plusword-is-even-better-thread-4?page=',\n",
    "            #'https://www.mumsnet.com/talk/_chat/4714295-plusword-new-thread-1?page=',\n",
    "            'https://www.mumsnet.com/talk/_chat/4765702-plusword-new-thread-2?page=']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3775fa-27b9-47a7-ac80-27678ae9a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url, max_pages, whole_post_list):\n",
    "    \n",
    "    # Increments through every page on website until it runs out for hits max_pages\n",
    "    for page_number in range(max_pages):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # gets request via bs4\n",
    "            r = requests.get(url + str(page_number))\n",
    "            soup = BeautifulSoup(r.content)\n",
    "            \n",
    "            # Finds original post on each page and splits it into metadata and post text\n",
    "            original_post = soup.find_all('div', class_= 'p-4 pb-1 pt-2.5 lg:py-2.5 mt-2.5 lg:mt-1.5 border-t border-b sm:border sm:rounded border-mumsnet-forest-border bg-mumsnet-forest dark:bg-mumsnet-forest-dark')\n",
    "            original_post_paragraphs=original_post[0].find_all('p')\n",
    "            \n",
    "            # converts to list\n",
    "            meta_data = original_post_paragraphs[0].getText().split()\n",
    "            \n",
    "            # removes fullstop in position 1\n",
    "            meta_data.pop(1)\n",
    "            \n",
    "            # converts text to list and then joins items together\n",
    "            post_text = original_post_paragraphs[1].getText().split()\n",
    "            post_text =' '.join(post_text)\n",
    "            \n",
    "            # Adds OP metadata and text together and adds together for OP on every page\n",
    "            meta_data.append(post_text)\n",
    "            whole_post = meta_data\n",
    "            whole_post_list.append(whole_post)\n",
    "            \n",
    "            # finds all non-OP post on page and gets data\n",
    "            posts= soup.find_all('div', class_=['lg:py-2.5 pt-2.5 pb-1 p-4 border-t border-b sm:border sm:rounded mt-1.5 overflow-x-hidden bg-white dark:bg-gray-800 border-gray-200',\n",
    "                                                'lg:py-2.5 pt-2.5 pb-1 p-4 border-t border-b sm:border sm:rounded mt-1.5 overflow-x-hidden bg-mumsnet-forest dark:bg-mumsnet-forest-dark border-mumsnet-forest-border'])\n",
    "            for post in posts:\n",
    "                post_info = post.getText().split()\n",
    "                \n",
    "                #first 4 items are meta data\n",
    "                meta_data = post_info[:4]\n",
    "                \n",
    "                #removes uneeded full stop\n",
    "                meta_data.pop(1)\n",
    "                \n",
    "               # joins post text together\n",
    "                post_text = post_info[4:]\n",
    "                post_text = ' '.join(post_text)\n",
    "                \n",
    "                \n",
    "                # appends metadata and text together and adds to list\n",
    "                meta_data.append(post_text)\n",
    "                whole_post = meta_data\n",
    "                whole_post_list.append(whole_post)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return whole_post_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2c62c-21ae-41dd-abfe-e31a4cc01b08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scraper initialization and df generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccb6c5-607b-4685-ba22-4739129f9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_post_list=[]\n",
    "\n",
    "# maxiumum number of pages in thread\n",
    "max_pages = 41\n",
    "\n",
    "for url in url_list:\n",
    "\n",
    "    whole_post_list = scraper(url, max_pages, whole_post_list)\n",
    "            \n",
    "df = pd.DataFrame(whole_post_list, columns=['user', 'date', 'time', 'text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7104a-9f1b-4b66-987f-4bc4a19aff4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Converts 'Today' and 'Yesterday to date values, creates and sorts by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584b88b-4419-47cb-9731-d25d46ef190d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['date'] = df['date'].str.replace('Yesterday', dt.datetime.strftime((dt.datetime.today() - dt.timedelta(days=1)), '%d/%m/%Y'))\n",
    "df['date'] = df['date'].str.replace('Today', dt.datetime.strftime(dt.datetime.today(), '%d/%m/%Y'))\n",
    "df['load_ts'] = df['date'] + ' ' + (df['time']+':00')\n",
    "df['load_ts'] = df['load_ts'] + '.000'\n",
    "df['load_ts'] = pd.to_datetime(df['load_ts'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "df = df.sort_values(by=['load_ts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4216210-d8d4-4408-b017-32bdbf1fd74e",
   "metadata": {},
   "source": [
    "### Extracts times from text and adds 00: to allow it to handle hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2efb6-5742-41ba-8b3e-76e9bd47755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] =df['text'].str.extract(r'(\\d*\\d:\\d\\d)')\n",
    "df = df.dropna(subset='text')\n",
    "df = df.copy()\n",
    "df['text'] =df['text'].str.replace(r'(^\\d:\\d\\d)', r'0\\1', regex=True)\n",
    "df['text'] = '00:' + df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcc181-ee50-459a-b718-16b82b91c9b8",
   "metadata": {},
   "source": [
    "### Drops duplicate entries for users on same date, drops columns and renames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b789493-e469-4e54-a415-30f581528120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df= df.drop_duplicates(subset=['user', 'date'])\n",
    "df = df.drop(columns=['date', 'time'])\n",
    "df = df.rename(columns={'text' : 'time'})\n",
    "df = df[['load_ts', 'time', 'user']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4af34-cb43-48ff-a097-f0d8d1aaee3a",
   "metadata": {},
   "source": [
    "### Loads in db data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e9e73-1829-4ed3-9580-4295c1d96d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mums = data_import('Mumsnet_Times')\n",
    "df_mums['load_ts'] = pd.to_datetime(df_mums['load_ts'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "df_mums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38aed0-857a-4727-bab9-5aa469edda3f",
   "metadata": {},
   "source": [
    "### Filters out rows that are already in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bb3d3-4cf8-48a1-ad44-8b79c5f96e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['load_ts', 'user'])\n",
    "df_mums = df_mums.set_index(['load_ts', 'user'])\n",
    "df = df[~df.index.isin(df_mums.index)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ef122-dfa6-409e-ad2e-e5046fae9fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Formats df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a2fb5-493b-408c-a51a-9c96d9628645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['load_ts'] = df['load_ts'].astype('str')\n",
    "df['load_ts'] = df['load_ts'] +'.000'\n",
    "df = df[['load_ts', 'time', 'user']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc618f-35b2-4b48-8646-b8a99230901a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not dataframe.empty:\n",
    "    data_export(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceabc69-c1a5-4660-9e3c-3560d1b6813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
